{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 소개\n",
    "* 조명 - 치즈케이크\n",
    "* 프로젝트명 - \n",
    "* 분석목적 - 최근 유행하는 치즈케이크 집의 분포를 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 조원소개와 역할 (사진포함)\n",
    "* 김다영\n",
    "* 김민기\n",
    "* 서준영\n",
    "* 이수진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 출처와 전처리 전의 데이터 구조 (테이블구조로 샘플수와 속성 설명 포함)\n",
    "## (1) 데이터 수집 방법 : 크롤링\n",
    "* 데이터 출처 : 인스타그램  \n",
    "인스타그램에서 #치즈케이크 해시태그로 검색하여 최근 게시물 데이터를 확보했다.  \n",
    "목표 샘플수는 10000개를 목표로 했으나, 인스타그램의 검색량 제한으로 중간에 멈췄다.  \n",
    "따라서 크롤링 과정을 2단계로 나누어 네 명이서 크롤링을 분담하였다.  \n",
    "특징: 위치 정보를 입력하지 않는 글은 저장되지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from urllib.parse import quote\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "#사전정보 입력\n",
    "handle = '' #인스타그램 아이디\n",
    "pwd = ''  #인스타그램 비밀번호\n",
    "\n",
    "keyword= '치즈케이크'   #검색할 키워드\n",
    "num_of_pages = 800 #몇 번 스크롤 할 건지; e.g. 200 --> 1730개 게시글 긁어짐\n",
    "driver_dir = 'c:/Temp/chromedriver.exe'    #크롬드라이버 경로\n",
    "k = 1 # k는 1~4로 각각 맡음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAW 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1단계 : 검색 페이지에서 글 URL 가져오기\n",
    "def InstagramUrlFromKeyword (browser,keyword,num_of_pagedowns):\n",
    "    keyword_url_encode=quote(keyword)\n",
    "    url='https://www.instagram.com/explore/tags/'+keyword+'/?hl=ko'\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "    login = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/nav/div[2]/div/div/div[3]/div/span/a[1]/button')\n",
    "    login.click()\n",
    "    time.sleep(3)\n",
    "    user_id = browser.find_element_by_class_name('_2hvTZ.pexuQ.zyHYP')\n",
    "    pw = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input')\n",
    "    user_id.send_keys(handle)\n",
    "    pw.send_keys(pwd)\n",
    "    button = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[4]/button')\n",
    "    button.click()\n",
    "    time.sleep(3)\n",
    "    arr_href=[]\n",
    "    body=browser.find_element_by_tag_name('body')\n",
    "    for i in range(num_of_pagedowns):\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(1)\n",
    "        post=browser.find_elements_by_class_name('v1Nh3')\n",
    "        for j in post:\n",
    "            href_str=j.find_element_by_css_selector('a').get_attribute('href')\n",
    "            arr_href.append(href_str)\n",
    "    return set(arr_href)    \n",
    "\n",
    "char = r'#[\\d\\w]+'\n",
    "char2 = re.compile('[^ 0-9a-zA-Zㄱ-ㅣ가-힣!#?]')\n",
    "\n",
    "#2단계 : URL 정보를 4명분으로 나누기\n",
    "arr = list(arr)\n",
    "text1 = arr[:threshold]\n",
    "text2 = arr[threshold: 2*threshold]\n",
    "text3 = arr[2*threshold: 3*threshold]\n",
    "text4 = arr[3*threshold:]\n",
    "texts = [text1, text2, text3, text4]\n",
    "\n",
    "for k in range(len(texts)):\n",
    "    with open('url_part' + str(k+1) + '.txt', 'w') as f:\n",
    "        for url in texts[k]:\n",
    "            f.write(url + '\\n')\n",
    "\n",
    "\n",
    "#3단계 : 글 URL에서 필요한 데이터 수집\n",
    "def IdHashTagFromInstagram(browser,url):\n",
    "    browser.get(url)\n",
    "    datetime = browser.find_element_by_class_name('_1o9PC.Nzb55').get_attribute('title')\n",
    "    href=browser.find_elements_by_class_name('C4VMK')\n",
    "    #포스트 내용(글, 태그)\n",
    "    total_hash_text=[]    \n",
    "    for i in range(0, len(href)):\n",
    "        hash_text= href[i].find_element_by_css_selector('span').text\n",
    "        total_hash_text.append(hash_text)\n",
    "    tags = re.findall(char, char2.sub(' ',str(total_hash_text)))\n",
    "    user_name = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/article/header/div[2]/div[1]/div[1]/h2/a').text\n",
    "    like_count = int(browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/article/div[2]/section[2]/div/div/button/span').text)\n",
    "    location = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/article/header/div[2]/div[2]/div[2]/a').text\n",
    "    loc_url = browser.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/div/article/header/div[2]/div[2]/div[2]/a').get_attribute('href')\n",
    "    return datetime, user_name, like_count, tags, location, url, loc_url\n",
    "\n",
    "#4단계 : 크롤링 시행\n",
    "browser = webdriver.Chrome(driver_dir)\n",
    "insta_df=pd.DataFrame(columns = {'Datetime', 'User_Name', 'Like_Count', 'Tags', 'Location','URL', 'Location_URL'})\n",
    "\n",
    "with open('url_part' + str(k) + '.txt', 'r') as f:\n",
    "    urls = f.readlines()\n",
    "for url in urls:\n",
    "    try:\n",
    "        datetime, user_name, like_count, tags, location, url, loc_url = IdHashTagFromInstagram(browser, url.strip('\\n'))\n",
    "        insta_df = insta_df.append({'Datetime': datetime, 'User_Name':user_name, 'Like_Count':like_count,'Tags': tags, 'Location' : location, 'URL':url,'Location_URL' : loc_url}, ignore_index = True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 후 네 개의 데이터 합치기\n",
    "insta_df_part1 = pd.read_csv('testtesttest1.csv', index_col=0, encoding='UTF-8')\n",
    "insta_df_part2 = pd.read_csv('testtesttest2.csv', index_col=0, encoding='UTF-8')\n",
    "insta_df_part3 = pd.read_csv('testtesttest3.csv', index_col=0, encoding='UTF-8')\n",
    "insta_df_part4 = pd.read_csv('testtesttest4.csv', index_col=0, encoding='UTF-8')\n",
    "insta_df_parts = pd.concat([insta_df_part1,insta_df_part2,insta_df_part3,insta_df_part4], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 크롤링 결과 수집된 데이터를 csv로 저장\n",
    "euc-kr로 저장하면 인코딩 문제로 오류나서 utf-8로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_df_parts.to_csv('rawdata_final.csv',mode='w', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 크롤링 완료 후 데이터가 잘 수집되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3159"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insta_df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 전처리 전의 데이터 구조\n",
    "* 샘플수 3159개\n",
    "* 수집 항목 : 작성날짜, 해시태그, 좋아요 수, 위치, 사용자 아이디, 지도url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location_URL</th>\n",
       "      <th>Tags</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Like_Count</th>\n",
       "      <th>URL</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/explore/locations/37...</td>\n",
       "      <td>['#고소함', '#흑임자', '#치즈케이크', '#할매입맛', '#크리스마스', ...</td>\n",
       "      <td>gosop_gosop</td>\n",
       "      <td>정자동 카페거리</td>\n",
       "      <td>37</td>\n",
       "      <td>https://www.instagram.com/p/B6ZmDGPnAsk/</td>\n",
       "      <td>2019년 12월 23일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/explore/locations/10...</td>\n",
       "      <td>['#달콤수다', '#데이트', '#이쁜그녀들', '#아메리카노', '#레드벨벳케이...</td>\n",
       "      <td>sunapingu</td>\n",
       "      <td>할리스커피(Hollys Coffee)</td>\n",
       "      <td>134</td>\n",
       "      <td>https://www.instagram.com/p/B6H-1k4ldab/</td>\n",
       "      <td>2019년 12월 16일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/explore/locations/15...</td>\n",
       "      <td>['#딸기와플', '#딸기', '#유달리', '#안산유달리', '#유달리안산점', ...</td>\n",
       "      <td>udally_ansan</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>29</td>\n",
       "      <td>https://www.instagram.com/p/B6UU24bFo8Q/</td>\n",
       "      <td>2019년 12월 21일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/explore/locations/18...</td>\n",
       "      <td>['#카누', '#윈터블렌드', '#한스오븐', '#마카롱', '#대전마카롱', '...</td>\n",
       "      <td>hans_oven</td>\n",
       "      <td>한스오븐 HAN’s OVEN</td>\n",
       "      <td>23</td>\n",
       "      <td>https://www.instagram.com/p/B6W82JwFMYi/</td>\n",
       "      <td>2019년 12월 22일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/explore/locations/17...</td>\n",
       "      <td>['#한남동카페', '#한남동', '#오지힐']</td>\n",
       "      <td>mocha_jun</td>\n",
       "      <td>오지힐 한남</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.instagram.com/p/B6DOfa8nNsA/</td>\n",
       "      <td>2019년 12월 14일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.instagram.com/explore/locations/43...</td>\n",
       "      <td>[]</td>\n",
       "      <td>cafe_sonane</td>\n",
       "      <td>소나네</td>\n",
       "      <td>49</td>\n",
       "      <td>https://www.instagram.com/p/B6Cz7DvDuXw/</td>\n",
       "      <td>2019년 12월 14일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.instagram.com/explore/locations/13...</td>\n",
       "      <td>[]</td>\n",
       "      <td>cafe.iam.autumn</td>\n",
       "      <td>I Am Autumn</td>\n",
       "      <td>96</td>\n",
       "      <td>https://www.instagram.com/p/B6m2YMvJsck/</td>\n",
       "      <td>2019년 12월 28일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.instagram.com/explore/locations/10...</td>\n",
       "      <td>['#버터힐', '#빠다힐', '#Butterhill', '#버티힐', '#치즈케이...</td>\n",
       "      <td>butter.hill</td>\n",
       "      <td>버터힐</td>\n",
       "      <td>44</td>\n",
       "      <td>https://www.instagram.com/p/B6UQ-7AJ2Kv/</td>\n",
       "      <td>2019년 12월 21일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.instagram.com/explore/locations/44...</td>\n",
       "      <td>['#데일리', '#아이폰11프로맥스', '#카페스타그램', '#인천카페', '#구...</td>\n",
       "      <td>ssu1215</td>\n",
       "      <td>휘게-HyggeCafe</td>\n",
       "      <td>71</td>\n",
       "      <td>https://www.instagram.com/p/B6K5c4lFosw/</td>\n",
       "      <td>2019년 12월 17일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.instagram.com/explore/locations/37...</td>\n",
       "      <td>['#제주카페스르륵', '#오션뷰카페', '#카페', '#바닐라라떼', '#치즈케이...</td>\n",
       "      <td>js_0421.th</td>\n",
       "      <td>제주카페 스르륵</td>\n",
       "      <td>17</td>\n",
       "      <td>https://www.instagram.com/p/B6kj8B_hzsL/</td>\n",
       "      <td>2019년 12월 27일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Location_URL  \\\n",
       "0  https://www.instagram.com/explore/locations/37...   \n",
       "1  https://www.instagram.com/explore/locations/10...   \n",
       "2  https://www.instagram.com/explore/locations/15...   \n",
       "3  https://www.instagram.com/explore/locations/18...   \n",
       "4  https://www.instagram.com/explore/locations/17...   \n",
       "5  https://www.instagram.com/explore/locations/43...   \n",
       "6  https://www.instagram.com/explore/locations/13...   \n",
       "7  https://www.instagram.com/explore/locations/10...   \n",
       "8  https://www.instagram.com/explore/locations/44...   \n",
       "9  https://www.instagram.com/explore/locations/37...   \n",
       "\n",
       "                                                Tags        User_Name  \\\n",
       "0  ['#고소함', '#흑임자', '#치즈케이크', '#할매입맛', '#크리스마스', ...      gosop_gosop   \n",
       "1  ['#달콤수다', '#데이트', '#이쁜그녀들', '#아메리카노', '#레드벨벳케이...        sunapingu   \n",
       "2  ['#딸기와플', '#딸기', '#유달리', '#안산유달리', '#유달리안산점', ...     udally_ansan   \n",
       "3  ['#카누', '#윈터블렌드', '#한스오븐', '#마카롱', '#대전마카롱', '...        hans_oven   \n",
       "4                         ['#한남동카페', '#한남동', '#오지힐']        mocha_jun   \n",
       "5                                                 []      cafe_sonane   \n",
       "6                                                 []  cafe.iam.autumn   \n",
       "7  ['#버터힐', '#빠다힐', '#Butterhill', '#버티힐', '#치즈케이...      butter.hill   \n",
       "8  ['#데일리', '#아이폰11프로맥스', '#카페스타그램', '#인천카페', '#구...          ssu1215   \n",
       "9  ['#제주카페스르륵', '#오션뷰카페', '#카페', '#바닐라라떼', '#치즈케이...       js_0421.th   \n",
       "\n",
       "               Location  Like_Count                                       URL  \\\n",
       "0              정자동 카페거리          37  https://www.instagram.com/p/B6ZmDGPnAsk/   \n",
       "1  할리스커피(Hollys Coffee)         134  https://www.instagram.com/p/B6H-1k4ldab/   \n",
       "2                안산 유달리          29  https://www.instagram.com/p/B6UU24bFo8Q/   \n",
       "3       한스오븐 HAN’s OVEN          23  https://www.instagram.com/p/B6W82JwFMYi/   \n",
       "4                오지힐 한남          12  https://www.instagram.com/p/B6DOfa8nNsA/   \n",
       "5                   소나네          49  https://www.instagram.com/p/B6Cz7DvDuXw/   \n",
       "6           I Am Autumn          96  https://www.instagram.com/p/B6m2YMvJsck/   \n",
       "7                   버터힐          44  https://www.instagram.com/p/B6UQ-7AJ2Kv/   \n",
       "8          휘게-HyggeCafe          71  https://www.instagram.com/p/B6K5c4lFosw/   \n",
       "9              제주카페 스르륵          17  https://www.instagram.com/p/B6kj8B_hzsL/   \n",
       "\n",
       "        Datetime  \n",
       "0  2019년 12월 23일  \n",
       "1  2019년 12월 16일  \n",
       "2  2019년 12월 21일  \n",
       "3  2019년 12월 22일  \n",
       "4  2019년 12월 14일  \n",
       "5  2019년 12월 14일  \n",
       "6  2019년 12월 28일  \n",
       "7  2019년 12월 21일  \n",
       "8  2019년 12월 17일  \n",
       "9  2019년 12월 27일  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insta_df_final[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 지도 API를 이용하여 위치정보 텍스트를 위도, 경도로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location 데이터를 리스트로 가져오기\n",
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "from urllib.parse import quote\n",
    "\n",
    "#insta_df_final.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "\n",
    "addr_list = []\n",
    "for k in insta_df_final['Location']:\n",
    "    addr_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정자동 카페거리',\n",
       " '할리스커피(Hollys Coffee)',\n",
       " '안산 유달리',\n",
       " '한스오븐 HAN’s OVEN',\n",
       " '오지힐 한남',\n",
       " '소나네',\n",
       " 'I Am Autumn',\n",
       " '버터힐',\n",
       " '휘게-HyggeCafe',\n",
       " '제주카페 스르륵',\n",
       " 'Anoseestudio',\n",
       " '빌리엔젤강남',\n",
       " '고고케이크',\n",
       " '스타벅스 스타필드 위례 1f R점',\n",
       " '레몬드 Lemoned',\n",
       " 'Busan, South Korea',\n",
       " 'Seoul, South Korea',\n",
       " 'Jeju',\n",
       " '동부이촌동',\n",
       " '버터힐']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addr_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주소로 좌표 얻기\n",
    "import json\n",
    "import requests\n",
    "MY_APP_KEY = '3123c75b5738c8b04d18ea0521ee9f02'\n",
    "addr_xyList = []\n",
    "\n",
    "for addr in addr_list:\n",
    "    url = 'https://dapi.kakao.com/v2/local/search/keyword.json?query='+ quote(addr)\n",
    "    headers = {\"Authorization\": 'KakaoAK ' + MY_APP_KEY}\n",
    "    tempDic = {\"addr\": addr, \"x\" : None, \"y\": None}\n",
    "    result = json.loads(str(requests.get(url, headers=headers).text))\n",
    "    address = result['documents']\n",
    "    if len(address) != 0:\n",
    "        x = float(address[0]['x'])\n",
    "        y = float(address[0]['y'])\n",
    "        tempDic['x'] = x\n",
    "        tempDic['y'] = y\n",
    "        addr_xyList.append(tempDic)\n",
    "    else:\n",
    "        url = 'https://dapi.kakao.com/v2/local/search/address.json?query='+ quote(addr)\n",
    "        headers = {\"Authorization\": 'KakaoAK ' + MY_APP_KEY}\n",
    "        tempDic = {\"addr\": addr, \"x\" : None, \"y\": None}\n",
    "        result = json.loads(str(requests.get(url, headers=headers).text))\n",
    "        address = result['documents']\n",
    "        if len(address) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            x = float(address[0]['x'])\n",
    "            y = float(address[0]['y'])\n",
    "            tempDic['x'] = x\n",
    "            tempDic['y'] = y\n",
    "            addr_xyList.append(tempDic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_df = pd.DataFrame(addr_xyList)\n",
    "freq_df = addr_df.groupby('addr').count().sort_values(by = 'x', ascending = False)\n",
    "freq_df.drop('x', axis = 1, inplace = True)\n",
    "freq_df.columns = ['freq']\n",
    "freq_df.reset_index(inplace = True)\n",
    "\n",
    "total_df = pd.merge(addr_df, freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>정자동 카페거리</td>\n",
       "      <td>127.106139</td>\n",
       "      <td>37.370151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>정자동 카페거리</td>\n",
       "      <td>127.106139</td>\n",
       "      <td>37.370151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>정자동 카페거리</td>\n",
       "      <td>127.106139</td>\n",
       "      <td>37.370151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>할리스커피(Hollys Coffee)</td>\n",
       "      <td>126.986810</td>\n",
       "      <td>37.569945</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>안산 유달리</td>\n",
       "      <td>126.829315</td>\n",
       "      <td>37.313383</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   addr           x          y  freq\n",
       "0              정자동 카페거리  127.106139  37.370151     3\n",
       "1              정자동 카페거리  127.106139  37.370151     3\n",
       "2              정자동 카페거리  127.106139  37.370151     3\n",
       "3  할리스커피(Hollys Coffee)  126.986810  37.569945     1\n",
       "4                안산 유달리  126.829315  37.313383    17\n",
       "5                안산 유달리  126.829315  37.313383    17\n",
       "6                안산 유달리  126.829315  37.313383    17\n",
       "7                안산 유달리  126.829315  37.313383    17\n",
       "8                안산 유달리  126.829315  37.313383    17\n",
       "9                안산 유달리  126.829315  37.313383    17"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#위도,경도 변환은 시간이 오래걸려 csv로 저장한다.\n",
    "total_df.to_csv('rawdata_final_total.csv',mode='w', encoding='UTF-8')\n",
    "#필요시 로드\n",
    "#total_df = pd.read_csv('rawdata_final_total.csv', index_col=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 분석방법, 흐름소개\n",
    "* 분석방법 : 크롤링, 빈도분석, 지도 API를 이용하여 위치정보 텍스트를 위도, 경도로 변환\n",
    "* 흐름 소개 (동영상, 각종코드 넣기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시연을 위해 크롤링으로 얻은 raw data 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insta_df_final = pd.read_csv('rawdata_final.csv', index_col=0, encoding='UTF-8')\n",
    "insta_df_final[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈도분석\n",
    "수집한 데이터가 좋아요 수, 위도, 경도를 제외하고는 모두 범주형 데이터이다.  \n",
    "해시태그에 대해 빈도분석을 진행해보았다.  \n",
    "--> 대체로 유의미한 데이터로 보인다. 키워드와 관련된 단어를 분석하는 워드클라우드를 적용해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#치즈케이크', 871),\n",
       " ('#카페', 172),\n",
       " ('#일상', 137),\n",
       " ('#디저트', 120),\n",
       " ('#카페투어', 107),\n",
       " ('#크리스마스', 105),\n",
       " ('#cafe', 94),\n",
       " ('#카페스타그램', 84),\n",
       " ('#cheesecake', 84),\n",
       " ('#케이크', 83),\n",
       " ('#디저트카페', 80),\n",
       " ('#커피', 77),\n",
       " ('#coffee', 77),\n",
       " ('#마카롱', 76),\n",
       " ('#먹스타그램', 75),\n",
       " ('#데일리', 75),\n",
       " ('#좋아요', 63),\n",
       " ('#daily', 54),\n",
       " ('#딸기케이크', 51),\n",
       " ('#dessert', 49),\n",
       " ('#맛집', 49),\n",
       " ('#맛스타그램', 49),\n",
       " ('#아메리카노', 48),\n",
       " ('#스콘', 46),\n",
       " ('#딸기', 46),\n",
       " ('#맞팔', 46),\n",
       " ('#디저트맛집', 46),\n",
       " ('#부산카페', 44),\n",
       " ('#크리스마스케이크', 43),\n",
       " ('#소통', 42),\n",
       " ('#티라미수', 40),\n",
       " ('#마카롱맛집', 39),\n",
       " ('#cake', 39),\n",
       " ('#예쁜카페', 37),\n",
       " ('#브라우니', 34),\n",
       " ('#메리크리스마스', 33),\n",
       " ('#먹방', 33),\n",
       " ('#팔로우', 31),\n",
       " ('#초코케이크', 31),\n",
       " ('#마들렌', 30),\n",
       " ('#제리치즈케이크', 29),\n",
       " ('#선물', 29),\n",
       " ('#macarons', 29),\n",
       " ('#좋반', 29),\n",
       " ('#밀크티', 28),\n",
       " ('#커피맛집', 28),\n",
       " ('#크리스마스이브', 28),\n",
       " ('#세종시마카롱', 28),\n",
       " ('#새롬동카페', 28),\n",
       " ('#신상카페', 27)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "insta_tags = []\n",
    "for tags in insta_df_final['Tags']:\n",
    "    tags = tags.strip('[').strip(']').replace(\"'\", \"\")\n",
    "    tag_list = tags.split(', ')\n",
    "    while '' in tag_list:\n",
    "        tag_list.remove('')\n",
    "    insta_tags.extend(tag_list)\n",
    "    \n",
    "c = Counter(insta_tags)\n",
    "    \n",
    "c.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 시각화 도구와 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Folium 라이브러리를 이용하여 여러 방법으로 데이터를 시각화한다.  \n",
    "이유: 지도 데이터에 위치정보를 시각화 하여 한눈에 어느지역에 분포되어 있는지 알아보기 쉽다\n",
    "* #치즈케이크 해시태그와 연관된 단어를 워드클라우드로 시각화한다.  \n",
    "치즈케이크와 함께 언급량이 많은 단어를 분석함으로써 치즈케이크를 찾는 사람들의 관심사를 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 분석 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Folium 을 이용한 지도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 서울시 구별 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요시 로드 (좌표변환)\n",
    "total_df = pd.read_csv('rawdata_final_total.csv', index_col=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (사전작업) 구별 히트맵 작성을 위해 구별로 구획을 나눈 데이터를 가져와 분류한다\n",
    "\n",
    "geo_path = 'skorea_municipalities_geo_simple.json'\n",
    "geo_str = json.load(open(geo_path, encoding='utf-8'))\n",
    "\n",
    "guList = []\n",
    "\n",
    "for k in addr_xyList:\n",
    "    \n",
    "    url = 'https://dapi.kakao.com/v2/local/geo/coord2regioncode.json?x=' + str(k['x']) + '&y=' + str(k['y'])\n",
    "    headers = {\"Authorization\": 'KakaoAK ' + MY_APP_KEY}\n",
    "\n",
    "    result = json.loads(str(requests.get(url, headers=headers).text))\n",
    "    address = result['documents'][0]['region_2depth_name']\n",
    "    guList.append(address)\n",
    "\n",
    "#구별 빈도수로 데이터프레임 생성\n",
    "gu_df = pd.DataFrame(columns = ['Gu', 'Frequency'])\n",
    "gu_df['Gu'] = [x['id'] for x in geo_str['features']]\n",
    "\n",
    "for k in range(len(gu_df)):\n",
    "    count = guList.count(gu_df['Gu'][k])\n",
    "    gu_df['Frequency'][k] = count   \n",
    "\n",
    "gu_df['Frequency'] = gu_df['Frequency'].astype(int)\n",
    "gu_df['Gu'] = gu_df['Gu'].astype(str)\n",
    "gu_df.set_index('Gu', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#구 분류 역시 시간이 오래걸려 csv로 저장한다.\n",
    "gu_df.to_csv('rawdata_final_total_gu.csv',mode='w', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요시 로드 (구 리스트)\n",
    "gu_df = pd.read_csv('rawdata_final_total_gu.csv', index_col=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도 파일 생성\n",
    "map = folium.Map(location=[37.5502, 126.982], zoom_start=11, tiles='Stamen Terrain')\n",
    "\n",
    "map.choropleth(geo_data = geo_str,\n",
    "               data = gu_df.Frequency,\n",
    "               columns = [gu_df.index, gu_df.Frequency],\n",
    "               fill_color = 'YlGnBu',\n",
    "               key_on = 'feature.id') #PuRd, YlGnBu\n",
    "\n",
    "map.save('cheesecake_heatmap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------여기까지 완성---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전국 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3159\n",
      "396\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'guList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-11560219f69e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddr_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mguList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcheesecake_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m37.514322572335935\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m127.06283102249932\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzoom_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'guList' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "cheesecake_map = folium.Map(location=[37.514322572335935, 127.06283102249932],zoom_start=10)\n",
    "for i in xyList:\n",
    "    x = i['x']\n",
    "    y = i['y']\n",
    "    folium.Marker([y, x],icon=folium.Icon(color='orange')).add_to(cheesecake_map)\n",
    "\n",
    "geo_path = 'skorea_municipalities_geo_simple.json'\n",
    "geo_str = json.load(open(geo_path, encoding='utf-8'))\n",
    "gu_df = pd.DataFrame(columns = ['Gu', 'Frequency'])\n",
    "gu_df['Gu'] = [x['id'] for x in geo_str['features']]\n",
    "\n",
    "for k in range(len(gu_df)):\n",
    "    count = guList.count(gu_df['Gu'][k])\n",
    "    gu_df['Frequency'][k] = count\n",
    "\n",
    "gu_df['Frequency'] = gu_df['Frequency'].astype(int)\n",
    "gu_df['Gu'] = gu_df['Gu'].astype(str)\n",
    "gu_df.set_index('Gu', inplace = True)\n",
    "\n",
    "\n",
    "map.choropleth(geo_data = geo_str,\n",
    "               data = gu_df.Frequency,\n",
    "               columns = [gu_df.index, gu_df.Frequency],\n",
    "               fill_color = 'YlGnBu',\n",
    "               key_on = 'feature.id') #PuRd, YlGnBu\n",
    "\n",
    "for i in xyList:\n",
    "    x = i['x']\n",
    "    y = i['y']\n",
    "    folium.Marker([y, x],icon=folium.Icon(color='orange')).add_to(map)\n",
    "\n",
    "map.save('cheesecake_seoul.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 워드클라우드 (미완성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw 데이터에서는 태그가 한 셀에 저장되어 있는데, 이를 쪼개어 태그 리스트로 저장한다.\n",
    "from collections import Counter\n",
    "\n",
    "insta_tags = []\n",
    "for tags in insta_df_final['Tags']:\n",
    "    tags = tags.strip('[').strip(']').replace(\"'\", \"\")\n",
    "    tag_list = tags.split(', ')\n",
    "    while '' in tag_list:\n",
    "        tag_list.remove('')\n",
    "    insta_tags.extend(tag_list)\n",
    "    \n",
    "c = Counter(insta_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 보완할 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 프로젝트 느낀점"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
