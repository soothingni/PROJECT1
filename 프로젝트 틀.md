# 1. 조명, 프로젝트명 (분석목적)

* 조명 - [?????????????????]

* 프로젝트명 - 유명 치즈케이크 맛집 찾기 
* 분석목적 - 치즈케이크를 좋아하는 사람들을 위한 치즈케이크 맛집 위치 알아보기



# 2. 조원소개와 역할 (사진포함)

* 김다영
* 김민기
* 서준영   
* 이수진



# 3. 데이터 출처와 전처리전의 데이터 구조(테이블구조로 샘플수와 속성 설명 포함)

* 데이터 출처 : 인스타그램
* 전처리전의 데이터 구조	
  * 샘플수  1102개



# 4. 분석방법, 흐름소개

* 분석방법 : 크롤링, 빈도분석, 지도 API를 이용하여 위치정보 텍스트를 위도, 경도로 변환

* 흐름 소개 (동영상, 각종코드 넣기)

  #### 1. 사전환경설정

  ```python
  from selenium import webdriver
  from urllib.parse import quote
  from selenium.webdriver.common.keys import Keys
  import time
  import pandas as pd 
  import re
  ```

  #### 2. 크롤링으로 RAW 데이터 수집하기

  ##### 2-1

  ```python
  handle = '' #인스타그램 아이디
  pwd = ''  #인스타그램 비밀번호
  
  keyword= '치즈케이크'   #검색할 키워드
  num_of_pages = 800 #몇 번 스크롤 할 건지; e.g. 200 --> 1730개 게시글 긁어짐
  ```

  ##### 2-2

  1단계 : 검색 페이지에서 글 URL 가져오기

  ```python
  def InstagramUrlFromKeyword (browser,keyword,num_of_pagedowns):
      keyword_url_encode=quote(keyword)
      url='https://www.instagram.com/explore/tags/'+keyword+'/?hl=ko'
      browser.get(url)
      time.sleep(3)
      login = browser.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[3]/div/span/a[1]/button')
      login.click()
      time.sleep(3)
      user_id = browser.find_element_by_class_name('_2hvTZ.pexuQ.zyHYP')
      pw = browser.find_element_by_xpath('//*[@id="react-root"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input')
      user_id.send_keys(handle)
      pw.send_keys(pwd)
      button = browser.find_element_by_xpath('//*[@id="react-root"]/section/main/div/article/div/div[1]/div/form/div[4]/button')
      button.click()
      time.sleep(3)
      arr_href=[]
      body=browser.find_element_by_tag_name('body')
      for i in range(num_of_pagedowns):
          body.send_keys(Keys.PAGE_DOWN)
          time.sleep(1)
          post=browser.find_elements_by_class_name('v1Nh3')
          for j in post:
              href_str=j.find_element_by_css_selector('a').get_attribute('href')
              arr_href.append(href_str)
      return set(arr_href)    
  
  char = r'#[\d\w]+'
  char2 = re.compile('[^ 0-9a-zA-Zㄱ-ㅣ가-힣!#?]')
  ```
  
  2단계 : 글 URL에서 필요한 데이터 수집
  
  ```python
  def IdHashTagFromInstagram(browser,url):
      browser.get(url)
      datetime = browser.find_element_by_class_name('_1o9PC.Nzb55').get_attribute('title')
      href=browser.find_elements_by_class_name('C4VMK')
      #포스트 내용(글, 태그)
      total_hash_text=[]    
      for i in range(0, len(href)):
          hash_text= href[i].find_element_by_css_selector('span').text
          total_hash_text.append(hash_text)
      tags = re.findall(char, char2.sub(' ',str(total_hash_text)))
      user_name = browser.find_element_by_xpath('//*[@id="react-root"]/section/main/div/div/article/header/div[2]/div[1]/div[1]/h2/a').text
      like_count = int(browser.find_element_by_xpath('//*[@id="react-root"]/section/main/div/div/article/div[2]/section[2]/div/div/button/span').text)
      location = browser.find_element_by_xpath('//*[@id="react-root"]/section/main/div/div/article/header/div[2]/div[2]/div[2]/a').text
      
      return datetime, user_name, like_count, tags, location
  ```
  
  ##### 2-3 크롤링 시행
  
  ```python
  insta_df = []
  df2 = ""
  c = ""
  
  browser= webdriver.Chrome('c:/Temp/chromedriver.exe')
  arr=InstagramUrlFromKeyword(browser,keyword,num_of_pages)
  insta_df=pd.DataFrame(columns = {'Datetime', 'User_Name', 'Like_Count', 'Tags', 'Location'})
  
  for url in arr:
          try:
              datetime, user_name, like_count, tags, location = IdHashTagFromInstagram(browser,url)
              insta_df = insta_df.append({'Datetime': datetime, 'User_Name':user_name, 'Like_Count':like_count,'Tags': tags, 'Location' : location}, ignore_index = True)
          except:
              continue
  ```
  
  ##### 2-4 크롤링 완료 후 데이터가 잘 수집되었는지 확인
  
  ```python
  len(arr)
  ->6219
  ```
  
  ```python
  insta_df
  ```
  
  |      | Location                  | Datetime         | Tags                                                         | Like_Count | User_Name       |
  | ---- | ------------------------- | ---------------- | ------------------------------------------------------------ | ---------- | --------------- |
  | 0    | 향리단커피                | 2019년 12월 22일 | [#향리단스콘, #향남맛집, #스콘, #스콘맛집, #화성카페, #향남카페, #수원카... | 48         | hrdcoffee       |
  | 1    | Changwon                  | 2019년 12월 30일 | []                                                           | 11         | staymood_candle |
  | 2    | The Cheesecake Factory    | 2019년 12월 17일 | [#대기20분, #줄줄이소세지, #하와이, #하와이신혼여행, #하와이맛집, #치즈케... | 35         | _dbddbd_        |
  | 3    | 인사이드커피-Insidecoffee | 2019년 12월 17일 | [#코흘리게, #못생긴, #나의중학생시절, #기억하는사람, #16년지기, #고니, ... | 38         | jojosuhyun      |
  | 4    | Seoul, South Korea        | 2019년 12월 30일 | [#월요일, #모닝커피, #커피빈, #치즈케이크, #셋이아닌둘이서, #커피스타그램,... | 21         | eunyoung3280    |
  | ...  | ...                       | ...              | ...                                                          | ...        | ...             |
  | 1098 | 은계지구                  | 2019년 12월 27일 | [#은계지구, #초밥, #허니콤보, #핫도그, #신전, #오돌뼈, #치즈케이크, #... | 41         | yang_hj2        |
  | 1099 | 얼스어스                  | 2019년 12월 18일 | [#얼스어스]                                                  | 21         | juny_hoy        |
  | 1100 | 스윗레시피 Sweet Recipe   | 2019년 12월 14일 | [#스윗레시피]                                                | 148        | sweet_0310      |
  | 1101 | 하우다 과자점             | 2019년 12월 25일 | []                                                           | 25         | hauda_jeju      |
  | 1102 | 동대문디자인플라자 DDP    | 2019년 12월 27일 | []                                                           | 14         | cafe_apotik     |
  
  1103 rows × 5 columns
  
  ##### 2-5 크롤링 결과 수집된 데이터를 csv로 저장
  
  euc-kr로 저장하면 인코딩 문제로 오류나서 utf-8로 저장함
  
  ```python
  #insta_df.to_csv(keyword + '_' + str(num_of_pages) + '.csv',mode='w', encoding='euc-kr')
  insta_df.to_csv(keyword + '_' + str(num_of_pages) + '.csv',mode='w', encoding='UTF-8')
  ```
  
  #### 3. 데이터 처리
  
  ##### 3-1 (참고) 시연을 위해 크롤링으로 얻은 raw data 로드
  
  ```python
  insta_df = pd.read_csv('rawdata.csv', index_col=0, encoding='UTF-8')
  insta_df
  ```
  
  ##### (참고) 빈도분석
  
  ```python
  from collections import Counter
  
  insta_str = []
  for tags in insta_df['Tags']:
      insta_str.extend(tags)
      
  c = Counter(insta_str)
  
  c.most_common(50)
  
  df2 = pd.DataFrame(columns = {'word', 'count'})
  for word in c:
      df2 = df2.append({'word':word, 'count':c[word]}, ignore_index = True)
  #결과 저장
  df2.to_csv(keyword + '_' + str(num_of_pages) + '_' + 'freq' + '.csv',mode='w', encoding='UTF-8')
  ```
  
  ##### (참고) url 컬럼 추가
  
  ```pyhton
  #치즈케이크 191230
  arr
  ```
  
  ```
  insta_df_urladd = insta_df
  ```
  
  ```
  insta_df_urladd
  ```
  
  |      | Location                  | Datetime         | Tags                                                         | Like_Count | User_Name       |
  | ---- | ------------------------- | ---------------- | ------------------------------------------------------------ | ---------- | --------------- |
  | 0    | 향리단커피                | 2019년 12월 22일 | [#향리단스콘, #향남맛집, #스콘, #스콘맛집, #화성카페, #향남카페, #수원카... | 48         | hrdcoffee       |
  | 1    | Changwon                  | 2019년 12월 30일 | []                                                           | 11         | staymood_candle |
  | 2    | The Cheesecake Factory    | 2019년 12월 17일 | [#대기20분, #줄줄이소세지, #하와이, #하와이신혼여행, #하와이맛집, #치즈케... | 35         | *dbddbd*        |
  | 3    | 인사이드커피-Insidecoffee | 2019년 12월 17일 | [#코흘리게, #못생긴, #나의중학생시절, #기억하는사람, #16년지기, #고니, ... | 38         | jojosuhyun      |
  | 4    | Seoul, South Korea        | 2019년 12월 30일 | [#월요일, #모닝커피, #커피빈, #치즈케이크, #셋이아닌둘이서, #커피스타그램,... | 21         | eunyoung3280    |
  | ...  | ...                       | ...              | ...                                                          | ...        | ...             |
  | 1098 | 은계지구                  | 2019년 12월 27일 | [#은계지구, #초밥, #허니콤보, #핫도그, #신전, #오돌뼈, #치즈케이크, #... | 41         | yang_hj2        |
  | 1099 | 얼스어스                  | 2019년 12월 18일 | [#얼스어스]                                                  | 21         | juny_hoy        |
  | 1100 | 스윗레시피 Sweet Recipe   | 2019년 12월 14일 | [#스윗레시피]                                                | 148        | sweet_0310      |
  | 1101 | 하우다 과자점             | 2019년 12월 25일 | []                                                           | 25         | hauda_jeju      |
  | 1102 | 동대문디자인플라자 DDP    | 2019년 12월 27일 | []                                                           | 14         | cafe_apotik     |
  
  ```python
  type(insta_df_urladd)
  -> pandas.core.frame.DataFrame
  ```
  
  ```python
  type(arr)
  ->set
  ```
  
  ```python
  import pandas as p
  urls = p.DataFrame(list(arr))
  ```
  
  ```python
  urls
  ```
  
  |      |                                          |
  | ---- | ---------------------------------------- |
  | 0    | https://www.instagram.com/p/B6XfP5YgO2g/ |
  | 1    | https://www.instagram.com/p/B6n6jBfFVNK/ |
  | 2    | https://www.instagram.com/p/B6po6sxghvm/ |
  | 3    | https://www.instagram.com/p/B6qIMZdprLK/ |
  | 4    | https://www.instagram.com/p/B6pOCC8jNaf/ |
  | ...  | ...                                      |
  | 6214 | https://www.instagram.com/p/B6j0btVH0iV/ |
  | 6215 | https://www.instagram.com/p/B6VS8qHBWmP/ |
  | 6216 | https://www.instagram.com/p/B6rdmlLJKjA/ |
  | 6217 | https://www.instagram.com/p/B6c7YbVhF7e/ |
  | 6218 | https://www.instagram.com/p/B6ddE8aJE-J/ |
  
  6219 rows × 1 columns
  
  ```pyhton
  insta_df_urladd["url"] = urls
  ```
  
  ```python
  insta_df_urladd
  ```
  
  

  ```python
  #csv 저장
  #insta_df.to_csv(keyword + '_' + str(num_of_pages) + '.csv',mode='w', encoding='euc-kr')
  insta_df_urladd.to_csv(keyword + '_' + str(num_of_pages) + '_urladded' + '.csv',mode='w', encoding='UTF-8')
  ```
  
  ##### 3-2 지도 API를 이용하여 위치정보 텍스트를 위도, 경도로 변환
  
  
  
  
  
  
  



# 5. 시각화 도구와 이유

* 
* 이유 : 한눈에 어느지역에 분포되어 있는지 알아보기 쉽기 때문에





# 6. 분석결과







# 7. 보완할점

* 인스타그램의 크롤링의 한계점으로 크롤링하던중에 인스타그램에서 크롤링을 막는 경우가 빈번하게 일어나다 보니 많은 양의 데이터를 추출할수없는 한계점이 나타남

* 또한, 최신데이터이기 때문에 추출해본결과 2주정도의 데이터(#치즈케이크) 밖에 추출하지 못함

  



# 8. 프로젝트 느낀점(조원별 소감)

* 김다영

* 김민기

* 서준영   

* 이수진

  



